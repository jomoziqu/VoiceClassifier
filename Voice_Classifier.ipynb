{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a7e2cd",
   "metadata": {},
   "source": [
    "## Voice Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61752d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2be14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file and extract features\n",
    "def load_and_extract_features(*audio_paths, max_length=None):\n",
    "    mfccs_list = []\n",
    "    pitch_list = []\n",
    "    formants_list = []\n",
    "\n",
    "    for audio_path in audio_paths:\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "        # Pad or truncate MFCCs to the maximum length\n",
    "        if max_length is not None:\n",
    "            if mfccs.shape[1] < max_length:\n",
    "                mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
    "            elif mfccs.shape[1] > max_length:\n",
    "                mfccs = mfccs[:, :max_length]\n",
    "\n",
    "        pitch = np.mean(librosa.yin(y, fmin=50, fmax=200))\n",
    "        formants = np.mean(librosa.effects.harmonic(y))\n",
    "\n",
    "        mfccs_list.append(mfccs)\n",
    "        pitch_list.append(pitch)\n",
    "        formants_list.append(formants)\n",
    "\n",
    "    return np.array(mfccs_list), np.array(pitch_list), np.array(formants_list)\n",
    "\n",
    "\n",
    "# max_length = max(X_human_male_mfccs.shape[1], X_human_female_mfccs.shape[1], X_ai_male_mfccs.shape[1], X_ai_female_mfccs.shape[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb91e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio paths\n",
    "human_male_audio_paths = [\n",
    "    'Human-voice2-m.mp3',\n",
    "    'Human-voice3-m.mp3',\n",
    "    'Human-voice4-m.mp3',\n",
    "    'Human-voice5-m.mp3',\n",
    "    'Human-4-m.mp3',\n",
    "    'Human-5-m.mp3',\n",
    "    'Human-6-m.mp3',\n",
    "    'Human-7-m.mp3'\n",
    "]\n",
    "\n",
    "human_female_audio_paths = [\n",
    "    'Human-voice6-f.mp3',\n",
    "    'Human-voice1-f.mp3',\n",
    "    'Human-voice (copy)-f.mp3',\n",
    "    # 'Human-1-f.aac',\n",
    "    'Human-2-f.mp3',\n",
    "    'Human-3-f.mp3',\n",
    "    'Human-8-f.mp3'\n",
    "]\n",
    "\n",
    "ai_male_audio_paths = [\n",
    "    'synthesize-m.mp3',\n",
    "    'synthesize-1-m.mp3',\n",
    "    'synthesize-2-m.mp3',\n",
    "    'synthesize-3-m.mp3',\n",
    "    'synthesize-4-m.mp3',\n",
    "    'synthesize-5-m.mp3',\n",
    "    'synthesize-6-m.mp3',\n",
    "    'synthesize-7-m.mp3',\n",
    "    'synthesize-8-m.mp3'\n",
    "]\n",
    "\n",
    "ai_female_audio_paths = [\n",
    "    'synthesize-f.mp3',\n",
    "    'synthesize-1-f.mp3',\n",
    "    'synthesize-2-f.mp3',\n",
    "    'synthesize-3-f.mp3',\n",
    "    'synthesize-4-f.mp3',\n",
    "    'synthesize-5-f.mp3',\n",
    "    'synthesize-6-f.mp3',\n",
    "    'synthesize-7-f.mp3',\n",
    "    'synthesize-8-f.mp3'\n",
    "]\n",
    "\n",
    "max_length = 500\n",
    "# Load and extract features from audio files with padding/truncation\n",
    "X_human_male_mfccs, _, _ = load_and_extract_features(*human_male_audio_paths, max_length=max_length)\n",
    "X_human_female_mfccs, _, _ = load_and_extract_features(*human_female_audio_paths, max_length=max_length)\n",
    "X_ai_male_mfccs, _, _ = load_and_extract_features(*ai_male_audio_paths, max_length=max_length)\n",
    "X_ai_female_mfccs, _, _ = load_and_extract_features(*ai_female_audio_paths, max_length=max_length)\n",
    "\n",
    "\n",
    "\n",
    "# Create labels\n",
    "y_human_male = np.zeros(len(human_male_audio_paths))  # Human male voice: 0\n",
    "y_human_female = np.ones(len(human_female_audio_paths))  # Human female voice: 1\n",
    "y_ai_male = np.ones(len(ai_male_audio_paths)) * 2  # AI male voice: 2\n",
    "y_ai_female = np.ones(len(ai_female_audio_paths)) * 3  # AI female voice: 3\n",
    "\n",
    "# Combine features and labels\n",
    "X = np.concatenate([X_human_male_mfccs, X_human_female_mfccs, X_ai_male_mfccs, X_ai_female_mfccs])\n",
    "y = np.concatenate([y_human_male, y_human_female, y_ai_male, y_ai_female])\n",
    "\n",
    "# Shuffle data\n",
    "random_indices = np.random.permutation(len(X))\n",
    "X = X[random_indices]\n",
    "y = y[random_indices]\n",
    "\n",
    "# Normalize MFCCs\n",
    "X_mfccs_normalized = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mfccs_normalized, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bea0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4903 - accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.1580 - accuracy: 0.4688\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8547 - accuracy: 0.5938\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6634 - accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5646 - accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4863 - accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4428 - accuracy: 0.7500\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4097 - accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3933 - accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3806 - accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3752 - accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3688 - accuracy: 0.7500\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3648 - accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3604 - accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3598 - accuracy: 0.7500\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3563 - accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3557 - accuracy: 0.7500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3543 - accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3527 - accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3529 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e6956fe7d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define deep neural network model\n",
    "model1 = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')  # 4 output classes\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model1.fit(X_mfccs_normalized, y, epochs=20, batch_size=32,) #validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3079bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted class: human male\n"
     ]
    }
   ],
   "source": [
    "# Load and extract features from a single audio file\n",
    "def load_and_extract_feature(audio_path, max_length=None):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Pad or truncate MFCCs to the maximum length\n",
    "    if max_length is not None:\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
    "        elif mfccs.shape[1] > max_length:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "\n",
    "    # Expand dimensions to match the expected input shape of the model\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Add batch dimension\n",
    "    return mfccs\n",
    "\n",
    "# Load and extract features from a single audio file\n",
    "audio_path = 'Human-4-m.mp3'\n",
    "max_length = 500  # Set the maximum length of MFCCs\n",
    "X_single_audio = load_and_extract_feature(audio_path, max_length=max_length)\n",
    "\n",
    "# Make prediction using the trained model\n",
    "predicted_label = model1.predict(X_single_audio)\n",
    "\n",
    "# Decode the predicted label (optional)\n",
    "class_names = ['human male', 'human female', 'robot male', 'robot female']\n",
    "predicted_class_index = np.argmax(predicted_label)\n",
    "predicted_class = class_names[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
